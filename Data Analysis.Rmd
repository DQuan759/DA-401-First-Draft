---
title: "Data Analysis"
author: "Derek"
date: '2023-03-07'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Import packages
```{r}
library(readxl)
library(lubridate)
library(zoo)
library(dplyr)
library(astsa)
library(caret)
library(ggplot2)
library(mosaic)
library(rpart.plot)
library(car)
library(gridExtra)
```

### Read in and transform data
```{r}
df = read.csv("Finaldf.csv")
df = subset(df, !(memory_num == 1 & storage_num == 1 & OS_num == 1 & graphics_num == 2 & processor_num == 27.7))
df$TFdiscomfort <- ifelse(df$Pdiscomfort > 0, 1, 0)
```

### Create training and test set from data
```{r}
part = createDataPartition(df$Pdiscomfort, p = 0.7, list=FALSE)
training = df[part,]
test = df[-part,]
```

### Define x and y variables in my model
```{r}
x = subset(training, select = c("memory_num", "storage_num", "graphics_num", "OS_num", "processor_num"))
y = as.factor(training$TFdiscomfort)
```

### Logistic regression

#### Empirical logit plot function
```{r}
emplogitplot1=function(formula,data=NULL,ngroups=3,breaks=NULL, yes=NULL,padj=TRUE,out=FALSE,showplot=TRUE,showline=TRUE,ylab="Log(Odds)",xlab=NULL,dotcol="black",linecol="blue",pch=16,main="",ylim=NULL,xlim=NULL,lty=1,lwd=1,cex=1){
  mod=glm(formula,family=binomial,data)
  newdata=mod$model[,1:2]
  oldnames=names(newdata)
  if(is.null(xlab)){xlab=oldnames[2]}   #Need a label for x-axis
  names(newdata)=c("Y","X")
  newdata=na.omit(newdata)      #get rid of NA cases for either variable
  #if needed find the value for "success"
  newdata$Y=factor(newdata$Y)
  if(is.null(yes)){yes=levels(newdata$Y)[2]}
  if(ngroups=="all"){breaks=unique(sort(c(newdata$X,min(newdata$X)-1)))}
  if(is.null(breaks)){
    breaks= quantile(newdata$X, probs = (0:ngroups)/ngroups)
    breaks[1] <- breaks[1]-1
  }
  ngroups=length(breaks)-1
  newdata$XGroups=cut(newdata$X,breaks=breaks,labels=1:ngroups)
  Cases=as.numeric(mosaic::tally(~XGroups,data=newdata))
  XMean=as.numeric(mosaic::mean(X~XGroups,data=newdata))
  XMin=as.numeric(mosaic::min(X~XGroups,data=newdata))
  XMax=as.numeric(mosaic::max(X~XGroups,data=newdata))
  NumYes=as.numeric(mosaic::sum((Y==yes)~XGroups,data=newdata))
  Prop=round(NumYes/Cases,3)
  AdjProp=round((NumYes+0.5)/(Cases+1),3)
  Logit=as.numeric(log(AdjProp/(1-AdjProp)))
  if(!padj){Logit=as.numeric(log(Prop/(1-Prop)))}
  if(showplot){plot(Logit~XMean,ylab=ylab,col=dotcol,pch=pch,
       ylim=ylim,xlim=xlim,xlab=xlab,cex=cex,main=main)
  if(showline){abline(lm(Logit~XMean),col=linecol,lty=lty,lwd=lwd)}}
  GroupData=data.frame(Group=1:ngroups,Cases,XMin,XMax,XMean,NumYes,Prop,AdjProp,Logit)
  if(out){return(GroupData)}
}
```

```{r}
logim1 = glm(TFdiscomfort ~ processor_num, data=training, family=binomial(logit))
summary(logim1)
densityplot(log(x$OS_num))
densityplot(x$OS_num)
```

#### Empirical logit plots
```{r}
par(mfrow=c(2,2))
emplogitplot1(TFdiscomfort~jitter(OS_num),ngroups=50,main="Empirical Logit for discomfort ~ OS",data=training)
emplogitplot1(TFdiscomfort~jitter(processor_num),ngroups=50,main="Empirical Logit for discomfort ~ Processor",data=training)
emplogitplot1(TFdiscomfort~jitter(log(graphics_num)),ngroups=50,main="Empirical Logit for discomfort ~ log(Graphics)",data=training)
emplogitplot1(TFdiscomfort~jitter(log(storage_num)),ngroups=50,main="Empirical Logit for discomfort ~ log(Storage)",data=training)

emplogitplot1(TFdiscomfort~jitter(memory_num),ngroups=50,main="Empirical Logit for discomfort ~ Memory",data=training)
```
#### Interaction plots
```{r}
m = mean(training$storage_num,na.rm=T)
trainingnonzero=transform(training,storagebin=as.numeric(storage_num>m))

LowTable=emplogitplot1(TFdiscomfort~jitter(OS_num),data=subset(trainingnonzero,storagebin=="0"), ngroups=5,showplot=FALSE,out=TRUE)

HighTable=emplogitplot1(TFdiscomfort~jitter(OS_num),data=subset(trainingnonzero,storagebin=="1"), ngroups=5,showplot=FALSE,out=TRUE)

plot(Logit~XMean,data=LowTable,ylab="Log(Odds of win the game)",xlab="bluemidwr",ylim=c(-30,30),xlim=c(-2,2),pch="L",col="blue")

abline(lm(Logit~XMean,data=LowTable),col="blue")

points(HighTable$XMean,HighTable$Logit,col="red",pch="H")

abline(lm(Logit~XMean,data=HighTable),col="red",lty=2)
```
#### Attempts for logistic regression
```{r}
m1 = train(x, y, 
           method = "glm",
           family = "binomial",
           trControl =  trainControl(method="repeatedcv", number=10))
m1
Predict1 <- predict(m1,newdata = test)
confusionMatrix(Predict1, as.factor(test$TFdiscomfort))
```

```{r}
newx = subset(x, select = c("storage_num","OS_num"))

m2 = train(newx, y, 
           method = "glm",
           family = "binomial",
           trControl =  trainControl(method="repeatedcv", number=10))
m2
Predict2 <- predict(m2,newdata = test)
confusionMatrix(Predict2, as.factor(test$TFdiscomfort))
```

```{r}
newx = subset(x, select = c("storage_num","OS_num"))
newx$storage_num = log(newx$storage_num)
m3 = train(newx, y, 
           method = "glm",
           family = "binomial",
           trControl =  trainControl(method="repeatedcv", number=10))
m3
Predict3 <- predict(m3,newdata = test)
confusionMatrix(Predict3, as.factor(test$TFdiscomfort))
```

```{r}
x$storage_num = log(x$storage_num)

m3 = train(x, y, 
           method = "glm",
           family = "binomial",
           trControl =  trainControl(method="repeatedcv", number=10))
m3
Predict3 <- predict(m3,newdata = test)
confusionMatrix(Predict3, as.factor(test$TFdiscomfort))
```

```{r}
p1 <- ggplot(varImp(m1))
p2 <- ggplot(varImp(m2))

grid.arrange(p1, p2, nrow = 2)
```

```{r}
logit1 = glm(TFdiscomfort ~ storage_num+graphics_num+OS_num+memory_num+processor_num, data=training, family=binomial(logit))
summary(logit1)

logit2 = glm(TFdiscomfort ~ storage_num+OS_num, data=training, family=binomial(logit))
summary(logit2)
```

#### Check for conditions
```{r}
vif(logit2)

par(mfrow=c(1,2))
plot(logit2, which = 4)
plot(logit2, which = 5)

qqPlot(logit2)
```

```{r}
myplot = plotPoints(jitter(TFdiscomfort) ~ storage_num, data=training, alpha=0.3, pch=19, cex=2, ylab="Mental Discomfort")
plot(myplot)
logit = glm(TFdiscomfort ~ storage_num, data=training, family=binomial(logit))
fit.outcome = makeFun(logit)
plotFun(fit.outcome(storage_num=x) ~ x, add=TRUE)
```


```{r}
library(scatterplot3d)

# Fit the logistic regression model
logit <- glm(TFdiscomfort ~ storage_num + OS_num + memory_num, data = training, family = binomial(logit))

# Create a 3D scatterplot of the predictor variables
scatterplot3d(training$storage_num, training$OS_num, training$memory_num, color = ifelse(training$TFdiscomfort == 1, "red", "black"), pch = 20, main = "3D Scatterplot")

# Add the fitted surface to the plot
fit.outcome <- makeFun(logit)
scatterplot3d(fit.outcome(storage_num = training$storage_num, OS_num = training$OS_num, memory_num = training$memory_num) ~ training$storage_num + training$OS_num + training$memory_num, add = TRUE, type = "h")
```
#### decision tree
```{r, eval=FALSE}
m2 = train(x, y, method = "rpart", trControl = trainControl(method="repeatedcv", number=10))
m2
Predict2 <- predict(m2,newdata = test)
confusionMatrix(Predict2, as.factor(test$TFdiscomfort))
prp(m2$finalModel)
```

#### random forest
```{r, eval=FALSE}
x <- na.omit(x)
y <- na.omit(y)

trctrl = trainControl(method = "oob")
m3 = train(x, y, 
           method = "rf",
           trControl = trctrl,
           tuneLength = 4,
           ntree = 250,
           importance = TRUE)
m3
Predict3 <- predict(m3,newdata = test)
confusionMatrix(Predict3, as.factor(test$TFdiscomfort))
```

#### knn
```{r, eval=FALSE}
trctrl = trainControl(method = "repeatedcv", 
                      number = 5, 
                      repeats = 1)
m4 = train(x, y,
           method = "knn", 
           trControl=trctrl,
           tuneLength = 30)
m4
Predict4 <- predict(m4,newdata = test)
confusionMatrix(Predict4, as.factor(test$TFdiscomfort))
```

```{r}
df <- df %>%
  mutate(month = month(ymd(Release.Date)), year = year(ymd(Release.Date)))

# Calculate season
df <- df %>%
  mutate(Date = case_when(
    month %in% 1:3 ~ paste0(year, ".1"),
    month %in% 4:6 ~ paste0(year, ".2"),
    month %in% 7:9 ~ paste0(year, ".3"),
    month %in% 10:12 ~ paste0(year, ".4")
  ))
```


```{r}
tsplot(df$Pdiscomfort,main = "Percent discomfort comments in the past decades")
```





```{r}
wss = data.frame(k=1:30,wss=sapply(1:30, function(k) kmeans(df[,7], k, nstart=10)$tot.withinss))
wss %>%
  ggplot(aes(x=k, y=wss))+
  geom_line()+
  ggtitle("Plot of Within Cluster Sum of Squares")+
  ylab("within cluster Sum of Squares")+
  xlab("k values")
```

```{r}
cluster = kmeans(df$Pdiscomfort, 15)
df$clusters = cluster$cluster

centers <- aggregate(Pdiscomfort ~ clusters, df, mean)

ggplot(df, aes(x = as.factor(clusters), y = Pdiscomfort, color = factor(clusters))) +
  geom_point() +
  geom_point(data = centers, aes(x = as.factor(clusters), y = Pdiscomfort), size = 5, shape = 16, fill = "white") +
  scale_color_discrete(labels = paste0("Cluster ", 1:10))+
  labs(title = "Percent discomfort comments by Cluster", x = "Clusters", y = "Percent of comments that have the keywords")
```

```{r}
dist_mat <- dist(df)

# Apply hierarchical clustering with complete linkage
hc <- hclust(dist_mat, method = "complete")

plot(hc)
```






























