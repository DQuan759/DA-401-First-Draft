---
title: "Data Analysis"
author: "Derek"
date: '2023-03-07'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Import packages
```{r}
library(caret)
library(mosaic)
library(car)
library(gridExtra)
library(kableExtra)
```

### Read in and transform data
```{r}
df = read.csv("Finaldf.csv")
df = subset(df, !(memory_num == 1 & storage_num == 1 & OS_num == 1 & graphics_num == 2 & processor_num == 27.7)) #exclude rows with missing values for all categories
df$TFdiscomfort = ifelse(df$Pdiscomfort > 0, 1, 0) #create a binary variable as the response variable
df$year = format(as.Date(df$release_date, format = "%d-%b-%y"), "%Y") #create a year variable
df$year = as.numeric(df$year) #convert the year variable into numeric
```

### Create training and test set from data
```{r}
set.seed(0)
part = createDataPartition(df$Pdiscomfort, p = 0.7, list=FALSE)
training = df[part,]
test = df[-part,]
```


### Logistic regression

#### Empirical logit plot function
```{r}
emplogitplot1=function(formula,data=NULL,ngroups=3,breaks=NULL, yes=NULL,padj=TRUE,out=FALSE,showplot=TRUE,showline=TRUE,ylab="Log(Odds)",xlab=NULL,dotcol="black",linecol="blue",pch=16,main="",ylim=NULL,xlim=NULL,lty=1,lwd=1,cex=1){
  mod=glm(formula,family=binomial,data)
  newdata=mod$model[,1:2]
  oldnames=names(newdata)
  if(is.null(xlab)){xlab=oldnames[2]}   #Need a label for x-axis
  names(newdata)=c("Y","X")
  newdata=na.omit(newdata)      #get rid of NA cases for either variable
  #if needed find the value for "success"
  newdata$Y=factor(newdata$Y)
  if(is.null(yes)){yes=levels(newdata$Y)[2]}
  if(ngroups=="all"){breaks=unique(sort(c(newdata$X,min(newdata$X)-1)))}
  if(is.null(breaks)){
    breaks= quantile(newdata$X, probs = (0:ngroups)/ngroups)
    breaks[1] <- breaks[1]-1
  }
  ngroups=length(breaks)-1
  newdata$XGroups=cut(newdata$X,breaks=breaks,labels=1:ngroups)
  Cases=as.numeric(mosaic::tally(~XGroups,data=newdata))
  XMean=as.numeric(mosaic::mean(X~XGroups,data=newdata))
  XMin=as.numeric(mosaic::min(X~XGroups,data=newdata))
  XMax=as.numeric(mosaic::max(X~XGroups,data=newdata))
  NumYes=as.numeric(mosaic::sum((Y==yes)~XGroups,data=newdata))
  Prop=round(NumYes/Cases,3)
  AdjProp=round((NumYes+0.5)/(Cases+1),3)
  Logit=as.numeric(log(AdjProp/(1-AdjProp)))
  if(!padj){Logit=as.numeric(log(Prop/(1-Prop)))}
  if(showplot){plot(Logit~XMean,ylab=ylab,col=dotcol,pch=pch,
       ylim=ylim,xlim=xlim,xlab=xlab,cex=cex,main=main)
  if(showline){abline(lm(Logit~XMean),col=linecol,lty=lty,lwd=lwd)}}
  GroupData=data.frame(Group=1:ngroups,Cases,XMin,XMax,XMean,NumYes,Prop,AdjProp,Logit)
  if(out){return(GroupData)}
}
```


#### Empirical logit plots
```{r}
#Without log transformation
par(mfrow=c(2,3))
emplogitplot1(TFdiscomfort~jitter(OS_num),ngroups=50,main="Logit plot for discomfort~OS",data=training)
emplogitplot1(TFdiscomfort~jitter(processor_num),ngroups=50,main="Logit plot for discomfort~CPU",data=training)
emplogitplot1(TFdiscomfort~jitter(graphics_num),ngroups=50,main="Logit plot for discomfort~GPU",data=training)
emplogitplot1(TFdiscomfort~jitter(storage_num),ngroups=50,main="Logit plot for discomfort~HD",data=training)
emplogitplot1(TFdiscomfort~jitter(memory_num),ngroups=50,main="Logit plot for discomfort~RAM",data=training)
emplogitplot1(TFdiscomfort~jitter(year),ngroups=50,main="Logit plot for discomfort~year",data=training)
```


```{r}
#Log transformed graphics and storage
par(mfrow=c(2,3))
emplogitplot1(TFdiscomfort~jitter(OS_num),ngroups=50,main="Logit plot for discomfort~OS",data=training)
emplogitplot1(TFdiscomfort~jitter(processor_num),ngroups=50,main="Logit plot for discomfort~CPU",data=training)
emplogitplot1(TFdiscomfort~jitter(log(graphics_num)),ngroups=50,main="Logit plot for discomfort~log(GPU)",data=training)
emplogitplot1(TFdiscomfort~jitter(log(storage_num)),ngroups=50,main="Logit plot for discomfort~log(HD)",data=training)
emplogitplot1(TFdiscomfort~jitter(memory_num),ngroups=50,main="Logit plot for discomfort~RAM",data=training)
emplogitplot1(TFdiscomfort~jitter(year),ngroups=50,main="Logit plot for discomfort~year",data=training)
```


#### Attempts for logistic regression

##### Logistic model with all system requirements variables
```{r}
sysx = subset(training, select = c("memory_num", "storage_num", "graphics_num", "OS_num", "processor_num")) #define x variable in model, use only system requirements in this trial
y = as.factor(training$TFdiscomfort) #define y variable in model

sysfullmod = train(sysx, y, 
                   method = "glm",
                   family = "binomial",
                   trControl =  trainControl(method="repeatedcv", number=10))
sysfullmod
Predictsysfull = predict(sysfullmod,newdata = test) #make predictions on the test set based on the model
(sys_matrix = confusionMatrix(Predictsysfull, as.factor(test$TFdiscomfort))) #confusion matrix of the prediction
```

##### Logistic model with year variable involved
```{r}
fullx = subset(training, select = c("memory_num", "storage_num", "graphics_num", "OS_num", "processor_num", "year")) #define x variable in model, add year in this trial

fullmod = train(fullx, y, 
                method = "glm",
                family = "binomial",
                trControl =  trainControl(method="repeatedcv", number=10))
fullmod
Predictfull <- predict(fullmod,newdata = test)
(year_matrix = confusionMatrix(Predictfull, as.factor(test$TFdiscomfort))) 
```


##### Logistic model with log transformed variables
```{r}
transformedx = fullx %>%
  mutate(storage_num = log(storage_num),
         graphics_num = log(graphics_num))

transformed_test = test %>% #also transform the variables in test set to ensure they have the same input variables when predicting
  mutate(storage_num = log(storage_num),
         graphics_num = log(graphics_num))

transformedfullmod = train(transformedx, y, 
                           method = "glm",
                           family = "binomial",
                           trControl =  trainControl(method="repeatedcv", number=10))
transformedfullmod
Predict_transformedfull <- predict(transformedfullmod,newdata = transformed_test)
(trans_matrix = confusionMatrix(Predict_transformedfull, as.factor(transformed_test$TFdiscomfort)))
```

##### Logistic model with interaction terms
```{r}
interaction_x = fullx %>%
  mutate(storage_graphics = storage_num*graphics_num,
         storage_processor = storage_num*processor_num,
         storage_OS = storage_num*OS_num,
         storage_memory = storage_num*memory_num,
         storage_year = storage_num*year,
         graphics_processor = graphics_num*processor_num,
         graphics_OS = graphics_num*OS_num,
         graphics_memory = graphics_num*memory_num,
         graphics_year = graphics_num*year,
         processor_OS = processor_num*OS_num,
         processor_memory = processor_num*memory_num,
         processor_year = processor_num*year,
         OS_memory = OS_num*memory_num,
         OS_year = OS_num*year,
         memory_year = memory_num*year)

interaction_test = test %>%
  mutate(storage_graphics = storage_num*graphics_num,
         storage_processor = storage_num*processor_num,
         storage_OS = storage_num*OS_num,
         storage_memory = storage_num*memory_num,
         storage_year = storage_num*year,
         graphics_processor = graphics_num*processor_num,
         graphics_OS = graphics_num*OS_num,
         graphics_memory = graphics_num*memory_num,
         graphics_year = graphics_num*year,
         processor_OS = processor_num*OS_num,
         processor_memory = processor_num*memory_num,
         processor_year = processor_num*year,
         OS_memory = OS_num*memory_num,
         OS_year = OS_num*year,
         memory_year = memory_num*year)

interact_fullmod = train(interaction_x, y, 
                           method = "glm",
                           family = "binomial",
                           trControl =  trainControl(method="repeatedcv", number=10))
interact_fullmod
Predict_interactfull <- predict(interact_fullmod,newdata = interaction_test)
(inter_matrix = confusionMatrix(Predict_interactfull, as.factor(interaction_test$TFdiscomfort)))
```


##### Logistic model with both transformation and interaction terms
```{r}
both_x = fullx %>%
  mutate(storage_num = log(storage_num),
         graphics_num = log(graphics_num),
         storage_graphics = storage_num*graphics_num,
         storage_processor = storage_num*processor_num,
         storage_OS = storage_num*OS_num,
         storage_memory = storage_num*memory_num,
         storage_year = storage_num*year,
         graphics_processor = graphics_num*processor_num,
         graphics_OS = graphics_num*OS_num,
         graphics_memory = graphics_num*memory_num,
         graphics_year = graphics_num*year,
         processor_OS = processor_num*OS_num,
         processor_memory = processor_num*memory_num,
         processor_year = processor_num*year,
         OS_memory = OS_num*memory_num,
         OS_year = OS_num*year,
         memory_year = memory_num*year)

both_test = test %>%
  mutate(storage_num = log(storage_num),
         graphics_num = log(graphics_num),
         storage_graphics = storage_num*graphics_num,
         storage_processor = storage_num*processor_num,
         storage_OS = storage_num*OS_num,
         storage_memory = storage_num*memory_num,
         storage_year = storage_num*year,
         graphics_processor = graphics_num*processor_num,
         graphics_OS = graphics_num*OS_num,
         graphics_memory = graphics_num*memory_num,
         graphics_year = graphics_num*year,
         processor_OS = processor_num*OS_num,
         processor_memory = processor_num*memory_num,
         processor_year = processor_num*year,
         OS_memory = OS_num*memory_num,
         OS_year = OS_num*year,
         memory_year = memory_num*year)

both_fullmod = train(both_x, y, 
                           method = "glm",
                           family = "binomial",
                           trControl =  trainControl(method="repeatedcv", number=10))
both_fullmod
Predict_bothfull <- predict(both_fullmod,newdata = both_test)
(both_matrix = confusionMatrix(Predict_bothfull, as.factor(both_test$TFdiscomfort)))
```



#### Model comparison
```{r}
Allmod = as.data.frame(matrix(0, ncol = 6, nrow = 3))
colnames(Allmod) = c("","sys_req_mod |", "year_mod |", "trans_mod |", "interact_mod |", "both_mod")
Allmod[1,1] = "Acuracy"
Allmod[2,1] = "Recall for discomfort"
Allmod[3,1] = "Recall for not discomfort"
Allmod[1,2] = as.numeric(sys_matrix$overall[1])
Allmod[2,2] = as.numeric(sys_matrix$byClass[2])
Allmod[3,2] = as.numeric(sys_matrix$byClass[1])
Allmod[1,3] = as.numeric(year_matrix$overall[1])
Allmod[2,3] = as.numeric(year_matrix$byClass[2])
Allmod[3,3] = as.numeric(year_matrix$byClass[1])
Allmod[1,4] = as.numeric(trans_matrix$overall[1])
Allmod[2,4] = as.numeric(trans_matrix$byClass[2])
Allmod[3,4] = as.numeric(trans_matrix$byClass[1])
Allmod[1,5] = as.numeric(inter_matrix$overall[1])
Allmod[2,5] = as.numeric(inter_matrix$byClass[2])
Allmod[3,5] = as.numeric(inter_matrix$byClass[1])
Allmod[1,6] = as.numeric(both_matrix$overall[1])
Allmod[2,6] = as.numeric(both_matrix$byClass[2])
Allmod[3,6] = as.numeric(both_matrix$byClass[1])

kable(Allmod)
```

#### Variable importance of model with transformation and model with interaction terms
```{r}
p1 <- ggplot(varImp(transformedfullmod), aes(x = Reorder, y = Importance)) +
      geom_bar(stat = "identity", fill = "steelblue") +
      ggtitle("Variable Importance for trans_mod") +
      xlab("Variable") + ylab("Importance")

p2 <- ggplot(varImp(both_fullmod), aes(x = Reorder, y = Importance)) +
      geom_bar(stat = "identity", fill = "steelblue") +
      ggtitle("Variable Importance for both_mod") +
      xlab("Variable") + ylab("Importance")

grid.arrange(p1, p2, nrow = 2)
```

#### Coefficients of variables in models
```{r}
trans_logit = glm(y ~ ., data=transformedx, family=binomial(logit))
summary(trans_logit)
```

```{r}
interact_logit = glm(y ~ ., data=interaction_x, family=binomial(logit))
summary(interact_logit)
```

#### Check for conditions
```{r}
vif(trans_logit) #check for multicollinearity

par(mfrow=c(1,2)) #check for outliers
plot(trans_logit, which = 4)
plot(trans_logit, which = 5)
```

```{r}
myplot = plotPoints(jitter(TFdiscomfort) ~ storage_num, data=training, alpha=0.3, pch=19, cex=2, ylab="Mental Discomfort", xlab="Storage")
plot(myplot)
logit = glm(TFdiscomfort ~ storage_num, data=training, family=binomial(logit))
fit.outcome = makeFun(logit)
plotFun(fit.outcome(storage_num=x) ~ x, add=TRUE)
```


```{r}
myplot = plotPoints(jitter(TFdiscomfort) ~ year, data=training, alpha=0.3, pch=19, cex=2, ylab="Mental Discomfort", xlab="Year")
plot(myplot)
logit = glm(TFdiscomfort ~ year, data=training, family=binomial(logit))
fit.outcome = makeFun(logit)
plotFun(fit.outcome(year=x) ~ x, add=TRUE)
```

```{r}
myplot = plotPoints(jitter(TFdiscomfort) ~ OS_num, data=training, alpha=0.3, pch=19, cex=2, ylab="Mental Discomfort", xlab="OS")
plot(myplot)
logit = glm(TFdiscomfort ~ OS_num, data=training, family=binomial(logit))
fit.outcome = makeFun(logit)
plotFun(fit.outcome(OS_num=x) ~ x, add=TRUE)
```

```{r}
myplot = plotPoints(jitter(TFdiscomfort) ~ graphics_num, data=training, alpha=0.3, pch=19, cex=2, ylab="Mental Discomfort", xlab="Graphics")
plot(myplot)
logit = glm(TFdiscomfort ~ graphics_num, data=training, family=binomial(logit))
fit.outcome = makeFun(logit)
plotFun(fit.outcome(graphics_num=x) ~ x, add=TRUE)
```


```{r}
myplot = plotPoints(jitter(TFdiscomfort) ~ processor_num, data=training, alpha=0.3, pch=19, cex=2, ylab="Mental Discomfort", xlab="Processor")
plot(myplot)
logit = glm(TFdiscomfort ~ processor_num, data=training, family=binomial(logit))
fit.outcome = makeFun(logit)
plotFun(fit.outcome(processor_num=x) ~ x, add=TRUE)
```


```{r}
myplot = plotPoints(jitter(TFdiscomfort) ~ OS_num, data=training, alpha=0.3, pch=19, cex=2, ylab="Mental Discomfort", xlab="Memory")
plot(myplot)
logit = glm(TFdiscomfort ~ OS_num, data=training, family=binomial(logit))
fit.outcome = makeFun(logit)
plotFun(fit.outcome(OS_num=x) ~ x, add=TRUE)
```


















